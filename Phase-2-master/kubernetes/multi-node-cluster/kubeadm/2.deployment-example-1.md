Reference: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

* Once the master and nodes are available run the below commands one by one and try to understand what is happening:

* **Command**: kubectl get nodes >>>> Check whether the nodes are ready or not.

* **Command**: kubectl get all >>> list all the deployments/rs/pods/svc

**picture-1**

![image](https://user-images.githubusercontent.com/24622526/44387273-986cdb80-a514-11e8-9137-3fc57f55858a.png)

* **Command**: kubectl get pods or kubectl get pod >>> check is there any pod available on the nodes.

## Step-1. Creating a Deployment/service:

* create deploy: kubectl create -f [nginx-deploy.yml](https://github.com/DevOpsPlatform/Phase-2/blob/master/kubernetes/yml/nginx-deploy.yml)
	
	kubectl get pods or kubectl get pod >>> check whether the pods created or not for this deployment.

	Result of the recently executed 3 commands.

* create service: kubectl create -f [nginx-service.yml](https://github.com/DevOpsPlatform/Phase-2/blob/master/kubernetes/yml/nginx-service.yml)

* kubectl get all >>> list all the deployments/rs/pods/svc

**picture-2**

![image](https://user-images.githubusercontent.com/24622526/44384307-4d9a9600-a50b-11e8-8db7-bb9044001179.png)

* **command**: kubectl get deployments (or) kubectl get deploy
	
**picture-3**
	
![image](https://user-images.githubusercontent.com/24622526/44384444-bb46c200-a50b-11e8-8176-bea54248a04d.png)

* deployment info: 

	* describe all the deployments: kubectl describe deploy
	* describe a single deployment: kubectl describe deploy nginx-deployment

**picture-4**

![image](https://user-images.githubusercontent.com/24622526/44387015-cbfb3600-a513-11e8-9be6-f2a250127d27.png)

* **All nodess**: kubectl get nodes

* **describe nodes**: 

	* **decribe all the nodes**: kubectl describe nodes
	* **describe a single node**: kubectl describe nodes "node-name"  (we can find an info what are all the pods are running in each node)
	

			root@k-master:~# kubectl get nodes
			NAME       STATUS    ROLES     AGE       VERSION
			k-master   Ready     master    28m       v1.11.2
			k-node-1   Ready     <none>    23m       v1.11.2
			k-node-2   Ready     <none>    11m       v1.11.2
			root@k-master:~# kubectl describe nodes k-node-1
			Name:               k-node-1
			Roles:              <none>
			Labels:             beta.kubernetes.io/arch=amd64
					    beta.kubernetes.io/os=linux
					    kubernetes.io/hostname=k-node-1
			Annotations:        kubeadm.alpha.kubernetes.io/cri-socket=/var/run/dockershim.sock
					    node.alpha.kubernetes.io/ttl=0
					    volumes.kubernetes.io/controller-managed-attach-detach=true
			CreationTimestamp:  Tue, 21 Aug 2018 07:21:36 +0000
			Taints:             <none>
			Unschedulable:      false
			Conditions:
			  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
			  ----                 ------  -----------------                 ------------------                ------                       -------
			  NetworkUnavailable   False   Tue, 21 Aug 2018 07:21:49 +0000   Tue, 21 Aug 2018 07:21:49 +0000   WeaveIsUp                    Weave pod has set this
			  OutOfDisk            False   Tue, 21 Aug 2018 07:44:38 +0000   Tue, 21 Aug 2018 07:21:36 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available
			  MemoryPressure       False   Tue, 21 Aug 2018 07:44:38 +0000   Tue, 21 Aug 2018 07:21:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
			  DiskPressure         False   Tue, 21 Aug 2018 07:44:38 +0000   Tue, 21 Aug 2018 07:21:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
			  PIDPressure          False   Tue, 21 Aug 2018 07:44:38 +0000   Tue, 21 Aug 2018 07:21:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
			  Ready                True    Tue, 21 Aug 2018 07:44:38 +0000   Tue, 21 Aug 2018 07:21:56 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
			Addresses:
			  InternalIP:  172.31.46.154
			  Hostname:    k-node-1
			Capacity:
			 cpu:                1
			 ephemeral-storage:  15181020Ki
			 hugepages-2Mi:      0
			 memory:             1014516Ki
			 pods:               110
			Allocatable:
			 cpu:                1
			 ephemeral-storage:  13990828009
			 hugepages-2Mi:      0
			 memory:             912116Ki
			 pods:               110
			System Info:
			 Machine ID:                 e8d61eaba5954831a078e7af8def0c46
			 System UUID:                EC24C848-CF74-AD85-144A-67EF0652213E
			 Boot ID:                    c33360d8-e69e-42cd-a352-026bf0799a27
			 Kernel Version:             4.4.0-1061-aws
			 OS Image:                   Ubuntu 16.04.4 LTS
			 Operating System:           linux
			 Architecture:               amd64
			 Container Runtime Version:  docker://17.3.2
			 Kubelet Version:            v1.11.2
			 Kube-Proxy Version:         v1.11.2
			Non-terminated Pods:         (5 in total)
			  Namespace                  Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits
			  ---------                  ----                                 ------------  ----------  ---------------  -------------
			  default                    nginx-deployment-67594d6bf6-8z5qq    0 (0%)        0 (0%)      0 (0%)           0 (0%)
			  default                    nginx-deployment-67594d6bf6-fp6vj    0 (0%)        0 (0%)      0 (0%)           0 (0%)
			  default                    nginx-deployment-67594d6bf6-wwvzx    0 (0%)        0 (0%)      0 (0%)           0 (0%)
			  kube-system                kube-proxy-8g277                     0 (0%)        0 (0%)      0 (0%)           0 (0%)
			  kube-system                weave-net-kgtb6                      20m (2%)      0 (0%)      0 (0%)           0 (0%)
			Allocated resources:
			  (Total limits may be over 100 percent, i.e., overcommitted.)
			  Resource  Requests  Limits
			  --------  --------  ------
			  cpu       20m (2%)  0 (0%)
			  memory    0 (0%)    0 (0%)
			Events:
			  Type    Reason                   Age                From                  Message
			  ----    ------                   ----               ----                  -------
			  Normal  Starting                 23m                kubelet, k-node-1     Starting kubelet.
			  Normal  NodeHasSufficientDisk    23m (x2 over 23m)  kubelet, k-node-1     Node k-node-1 status is now: NodeHasSufficientDisk
			  Normal  NodeHasSufficientMemory  23m (x2 over 23m)  kubelet, k-node-1     Node k-node-1 status is now: NodeHasSufficientMemory
			  Normal  NodeHasNoDiskPressure    23m (x2 over 23m)  kubelet, k-node-1     Node k-node-1 status is now: NodeHasNoDiskPressure
			  Normal  NodeHasSufficientPID     23m (x2 over 23m)  kubelet, k-node-1     Node k-node-1 status is now: NodeHasSufficientPID
			  Normal  NodeAllocatableEnforced  23m                kubelet, k-node-1     Updated Node Allocatable limit across pods
			  Normal  Starting                 23m                kube-proxy, k-node-1  Starting kube-proxy.
			  Normal  NodeReady                22m                kubelet, k-node-1     Node k-node-1 status is now: NodeReady
			root@k-master:~#

* **All pods**: kubectl get pods

**picture-5**

![image](https://user-images.githubusercontent.com/24622526/44387420-01545380-a515-11e8-9a6f-f839a217f148.png)

* **describe pods**: 

	* **describe all the pods**: kubectl describe pods
	* **describe a single pod**: kubectl describe pods/pod-name  (we can find an info related on which node this pod is running)

**picture-6**

![image](https://user-images.githubusercontent.com/24622526/44387618-87709a00-a515-11e8-855c-bfd9b37e5724.png)


#### When you inspect the Deployments in your cluster, the following fields are displayed:

* **command**: kubectl get deployments or kubectl get deploy

**picture-7**

![image](https://user-images.githubusercontent.com/24622526/44384444-bb46c200-a50b-11e8-8176-bea54248a04d.png)


* ***NAME*** lists the names of the Deployments in the cluster.
* ***DESIRED*** displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.
* ***CURRENT*** displays how many replicas are currently running.
* ***UP-TO-DATE*** displays the number of replicas that have been updated to achieve the desired state.
* ***AVAILABLE*** displays how many replicas of the application are available to your users.
* ***AGE*** displays the amount of time that the application has been running.

* **Check the statu of deployment**: kubectl rollout status deployment/nginx-deployment (or) kubectl rollout status deploy/nginx-deployment

**picture-8**

![image](https://user-images.githubusercontent.com/24622526/44384682-85560d80-a50c-11e8-89a6-3610b17aa320.png)

* To see the ReplicaSet (rs) created by the deployment, run the command: kubectl get rs:

**picture-9**

![image](https://user-images.githubusercontent.com/24622526/44384751-bcc4ba00-a50c-11e8-81c8-30ed320f9aaf.png)

* Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]. The hash value is automatically generated when the Deployment is created.

* To see the labels automatically generated for each pod, run the command "kubectl get pods --show-labels". The following output is returned:

**picture-10**

![image](https://user-images.githubusercontent.com/24622526/44384844-10370800-a50d-11e8-9cd5-8382b17684c9.png)

* The created ReplicaSet ensures that there are three nginx Pods running at all times.

## Step-2. Updating a Deployment

* Suppose that you now want to update the nginx Pods to use the nginx:1.9.1 image instead of the nginx:1.7.9 image.

* **Command**: kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1

**picture-11**

![image](https://user-images.githubusercontent.com/24622526/44388814-9efd5200-a518-11e8-98dd-400de3f25335.png)

* **command**(you can edit the deployment with right image tag version): kubectl edit deployment/nginx-deployment 
	* Once you run this command, deployment file will be opened, you can edit the image version here and save the file.
	
**picture-12**

![image](https://user-images.githubusercontent.com/24622526/44388940-03201600-a519-11e8-83c7-062b958c69be.png)

* see the rollout status, run: kubectl rollout status deployment/nginx-deployment

**picture-13**

![image](https://user-images.githubusercontent.com/24622526/44389075-67db7080-a519-11e8-94ad-bb30c9132faf.png)

* Run the below **commands**: the Deployment updated the Pods by creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.

	* kubectl get deploy
	* kubectl get rs
	* kubectl get pods
	
**picture-14**

![image](https://user-images.githubusercontent.com/24622526/44389302-fd770000-a519-11e8-895d-a37e943ee7fa.png)

* As per the below **picture-15**, one of the pod is rinning on ***k-node-2***. I am going to stop ***k-node-2*** instance in AWS. See **picture-16**, to **picture-20**.

 **picture-15** - Pod is running on ***k-node-2***.
 
 ![image](https://user-images.githubusercontent.com/24622526/44393944-7f205b00-a525-11e8-9131-6636d1cead50.png)

**picture-16** - Stopped an instance ***k-node-2***.

![image](https://user-images.githubusercontent.com/24622526/44394267-5187e180-a526-11e8-8d65-d82e3a357a5e.png)

**picture-17** - before & after stopped the nodes list in the cluster, and status of ***k-node-2*** is **NotReady**.

![image](https://user-images.githubusercontent.com/24622526/44394387-9ca1f480-a526-11e8-89eb-ea51e8813a1a.png)

**picture-18** (below image)- Automatically the new nodes(new containers with new lables) are creating and assiggning to other active node. See the **picture-19** to know the status of old nodes. See the **picture-20** where exactly the new pods are nunning. 

![image](https://user-images.githubusercontent.com/24622526/44394516-ea1e6180-a526-11e8-959b-deee4114fe75.png)

**picture-19** - Old nodes are terminating, reason is *NodeLost*.

![image](https://user-images.githubusercontent.com/24622526/44394571-133ef200-a527-11e8-9413-7347fb801de6.png)

![image](https://user-images.githubusercontent.com/24622526/44395193-a9274c80-a528-11e8-9666-07fd026afb5d.png)

**picture-20** - new nodes are automatically created in **k-node-1**

![image](https://user-images.githubusercontent.com/24622526/44394663-4c776200-a527-11e8-9e58-8d524b61d159.png)

**picture-21** - after sometime

![image](https://user-images.githubusercontent.com/24622526/44395243-ccea9280-a528-11e8-9e54-c8a245268b59.png)




